#!/usr/bin/env python3
"""
ReviewAgent - Night 37 Implementation
Runs Pytest in Cloud Build and loops back to DevAgent on failure
"""

from fastapi import FastAPI, HTTPException, Depends
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel, Field
import httpx
import os
import tempfile
import shutil
import subprocess
import uuid
from typing import List, Dict, Any, Optional, Literal
from datetime import datetime
import logging
from contextlib import asynccontextmanager
from pathlib import Path

# Import shared components
import sys
sys.path.append(os.path.join(os.path.dirname(__file__), '..', 'shared'))
from tenant_db import TenantDatabase, TenantContext, get_tenant_context_from_headers
from github_integration import (
    create_github_integration, ReviewComment
)

# Configure logging
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# Pydantic Models
class GeneratedCodeFile(BaseModel):
    """Model for a generated code file from DevAgent"""
    filename: str
    content: str
    file_type: str
    language: str
    size_bytes: int
    functions: List[str] = Field(default_factory=list)
    imports: List[str] = Field(default_factory=list)

class CodeReviewRequest(BaseModel):
    """Request model for code review"""
    project_id: str = Field(..., description="Project ID")
    module_name: str = Field(..., description="Name of the module")
    generated_files: List[GeneratedCodeFile] = Field(..., description="Files generated by DevAgent")
    dev_agent_request_id: Optional[str] = Field(None, description="Original DevAgent request ID for feedback loop")
    review_type: Literal["full", "syntax", "tests_only"] = Field(default="full", description="Type of review")
    cloud_build_config: Optional[Dict[str, Any]] = Field(None, description="Cloud Build configuration")

class TestResult(BaseModel):
    """Model for individual test result"""
    test_name: str
    status: Literal["passed", "failed", "skipped", "error"]
    duration: float
    error_message: Optional[str] = None
    traceback: Optional[str] = None
    assertion_failures: List[str] = Field(default_factory=list)

class PyTestResults(BaseModel):
    """Model for pytest execution results"""
    total_tests: int
    passed: int
    failed: int
    skipped: int
    errors: int
    coverage_percentage: Optional[float] = None
    duration: float
    test_results: List[TestResult]
    exit_code: int
    output: str
    error_output: str

class CloudBuildResult(BaseModel):
    """Model for Cloud Build execution result"""
    build_id: str
    status: Literal["success", "failure", "timeout", "cancelled"]
    duration: float
    log_url: str
    pytest_results: Optional[PyTestResults] = None
    artifacts: List[str] = Field(default_factory=list)

class ReviewFeedback(BaseModel):
    """Model for feedback to DevAgent"""
    review_passed: bool
    issues_found: List[str]
    test_failures: List[TestResult]
    suggestions: List[str]
    code_quality_score: float
    retry_recommended: bool
    fixed_code: Optional[List[GeneratedCodeFile]] = None

class CodeReviewResult(BaseModel):
    """Result model for code review"""
    review_id: str
    project_id: str
    module_name: str
    review_status: Literal["passed", "failed", "error"]
    cloud_build_result: Optional[CloudBuildResult] = None
    feedback: ReviewFeedback
    review_duration: float
    created_at: datetime
    retry_count: int = 0

class ReviewAgent:
    """Agent for reviewing generated code and running tests in Cloud Build"""
    
    def __init__(self):
        self.tenant_db = TenantDatabase()
        self.dev_agent_url = os.getenv("DEV_AGENT_URL", "http://dev-agent:8083")
        self.google_cloud_project = os.getenv("GOOGLE_CLOUD_PROJECT", "saas-factory-prod")
        self.cloud_build_region = os.getenv("CLOUD_BUILD_REGION", "us-central1")
        self.max_retry_attempts = int(os.getenv("MAX_REVIEW_RETRIES", "3"))
        self.github_integration = create_github_integration()
        
        # Auto-comment settings
        self.auto_comment_enabled = os.getenv("ENABLE_AUTO_COMMENT", "false").lower() == "true"
        
        # Cloud Build configuration template
        self.cloud_build_template = {
            "steps": [
                {
                    "name": "python:3.11-slim",
                    "entrypoint": "bash",
                    "args": [
                        "-c",
                        """
                        pip install pytest pytest-cov pytest-xdist asyncpg requests
                        cd /workspace
                        python -m pytest --tb=short --cov=. --cov-report=xml --cov-report=term-missing -v tests/
                        """
                    ]
                }
            ],
            "timeout": "600s",
            "options": {
                "logging": "CLOUD_LOGGING_ONLY"
            }
        }
    
    async def setup_test_workspace(self, files: List[GeneratedCodeFile]) -> str:
        """Setup temporary workspace with generated files for testing"""
        workspace_dir = tempfile.mkdtemp(prefix="review_workspace_")
        
        try:
            # Create directory structure
            src_dir = Path(workspace_dir) / "src"
            tests_dir = Path(workspace_dir) / "tests"
            src_dir.mkdir(exist_ok=True)
            tests_dir.mkdir(exist_ok=True)
            
            # Write generated files
            for file in files:
                if file.file_type == "source":
                    file_path = src_dir / file.filename
                elif file.file_type == "test":
                    file_path = tests_dir / file.filename
                else:
                    file_path = Path(workspace_dir) / file.filename
                
                file_path.parent.mkdir(parents=True, exist_ok=True)
                with open(file_path, 'w') as f:
                    f.write(file.content)
            
            # Create requirements.txt if not present
            req_file = Path(workspace_dir) / "requirements.txt"
            if not req_file.exists():
                requirements = [
                    "pytest>=7.0.0",
                    "pytest-cov>=4.0.0", 
                    "pytest-xdist>=3.0.0",
                    "fastapi>=0.104.0",
                    "pydantic>=2.0.0",
                    "asyncpg>=0.29.0",
                    "httpx>=0.25.0"
                ]
                
                # Extract imports from files to add as requirements
                for file in files:
                    for imp in file.imports:
                        if imp not in requirements and not imp.startswith('.'):
                            requirements.append(imp)
                
                with open(req_file, 'w') as f:
                    f.write('\n'.join(requirements))
            
            # Create pytest configuration
            pytest_config = Path(workspace_dir) / "pytest.ini"
            with open(pytest_config, 'w') as f:
                f.write("""[tool:pytest]
testpaths = tests
python_files = test_*.py *_test.py
python_classes = Test*
python_functions = test_*
addopts = --strict-markers --tb=short
markers =
    unit: Unit tests
    integration: Integration tests
    slow: Slow running tests
""")
            
            return workspace_dir
            
        except Exception as e:
            # Cleanup on error
            shutil.rmtree(workspace_dir, ignore_errors=True)
            raise Exception(f"Failed to setup test workspace: {e}")
    
    def parse_pytest_output(self, output: str, error_output: str, exit_code: int) -> PyTestResults:
        """Parse pytest output to extract test results"""
        results = PyTestResults(
            total_tests=0,
            passed=0,
            failed=0,
            skipped=0,
            errors=0,
            coverage_percentage=None,
            duration=0.0,
            test_results=[],
            exit_code=exit_code,
            output=output,
            error_output=error_output
        )
        
        # Parse test summary from output
        lines = output.split('\n')
        test_results = []
        
        for line in lines:
            # Parse individual test results
            if '::' in line and any(status in line for status in ['PASSED', 'FAILED', 'SKIPPED', 'ERROR']):
                parts = line.strip().split(' ')
                if len(parts) >= 2:
                    test_name = parts[0].split('::')[-1]
                    status_part = [p for p in parts if p in ['PASSED', 'FAILED', 'SKIPPED', 'ERROR']]
                    
                    if status_part:
                        status = status_part[0].lower()
                        test_result = TestResult(
                            test_name=test_name,
                            status=status,
                            duration=0.0  # Would need more detailed parsing for duration
                        )
                        
                        # Extract error message if test failed
                        if status in ['failed', 'error']:
                            # Look for error details in following lines
                            for i, next_line in enumerate(lines[lines.index(line)+1:], 1):
                                if next_line.strip().startswith('E '):
                                    test_result.error_message = next_line.strip()[2:]
                                    break
                                elif i > 10:  # Don't search too far
                                    break
                        
                        test_results.append(test_result)
            
            # Parse summary statistics
            elif '=' in line and any(word in line for word in ['passed', 'failed', 'skipped', 'error']):
                if 'passed' in line:
                    try:
                        results.passed = int(line.split('passed')[0].strip().split()[-1])
                    except (ValueError, IndexError):
                        pass
                if 'failed' in line:
                    try:
                        results.failed = int(line.split('failed')[0].strip().split()[-1])
                    except (ValueError, IndexError):
                        pass
                if 'skipped' in line:
                    try:
                        results.skipped = int(line.split('skipped')[0].strip().split()[-1])
                    except (ValueError, IndexError):
                        pass
            
            # Parse coverage percentage
            elif 'TOTAL' in line and '%' in line:
                try:
                    coverage_part = line.split('%')[0].strip().split()[-1]
                    results.coverage_percentage = float(coverage_part)
                except (ValueError, IndexError):
                    pass
            
            # Parse total duration
            elif 'seconds' in line and '=' in line:
                try:
                    duration_part = line.split('seconds')[0].strip().split()[-1]
                    results.duration = float(duration_part)
                except (ValueError, IndexError):
                    pass
        
        results.test_results = test_results
        results.total_tests = len(test_results)
        
        return results
    
    async def run_pytest_locally(self, workspace_dir: str) -> PyTestResults:
        """Run pytest locally as fallback when Cloud Build is unavailable"""
        logger.info(f"Running pytest locally in {workspace_dir}")
        
        try:
            # Install requirements
            subprocess.run([
                "pip", "install", "-r", f"{workspace_dir}/requirements.txt"
            ], check=True, capture_output=True, text=True, cwd=workspace_dir)
            
            # Run pytest
            result = subprocess.run([
                "python", "-m", "pytest", 
                "--tb=short", 
                "--cov=src", 
                "--cov-report=term-missing",
                "--cov-report=xml",
                "-v",
                "tests/"
            ], capture_output=True, text=True, cwd=workspace_dir)
            
            pytest_results = self.parse_pytest_output(
                result.stdout, 
                result.stderr, 
                result.returncode
            )
            
            logger.info(f"Local pytest completed: {pytest_results.passed} passed, {pytest_results.failed} failed")
            return pytest_results
            
        except subprocess.CalledProcessError as e:
            logger.error(f"Pytest execution failed: {e}")
            return PyTestResults(
                total_tests=0,
                passed=0,
                failed=1,
                skipped=0,
                errors=1,
                duration=0.0,
                test_results=[TestResult(
                    test_name="pytest_execution",
                    status="error",
                    duration=0.0,
                    error_message=f"Pytest execution failed: {e}"
                )],
                exit_code=e.returncode,
                output=e.stdout if hasattr(e, 'stdout') else "",
                error_output=e.stderr if hasattr(e, 'stderr') else str(e)
            )
        except Exception as e:
            logger.error(f"Unexpected error running pytest: {e}")
            return PyTestResults(
                total_tests=0,
                passed=0,
                failed=1,
                skipped=0,
                errors=1,
                duration=0.0,
                test_results=[TestResult(
                    test_name="pytest_setup",
                    status="error",
                    duration=0.0,
                    error_message=f"Setup error: {e}"
                )],
                exit_code=1,
                output="",
                error_output=str(e)
            )
    
    async def run_pytest_cloud_build(self, workspace_dir: str, build_config: Dict[str, Any]) -> CloudBuildResult:
        """Run pytest in Google Cloud Build"""
        logger.info("Running pytest in Google Cloud Build")
        
        try:
            # For now, fall back to local execution
            # In production, this would:
            # 1. Create Cloud Build configuration
            # 2. Upload workspace to Cloud Storage
            # 3. Trigger Cloud Build
            # 4. Monitor build status
            # 5. Download results
            
            pytest_results = await self.run_pytest_locally(workspace_dir)
            
            # Mock Cloud Build result
            build_result = CloudBuildResult(
                build_id=f"build-{uuid.uuid4()}",
                status="success" if pytest_results.exit_code == 0 else "failure",
                duration=pytest_results.duration,
                log_url=f"https://console.cloud.google.com/cloud-build/builds/build-{uuid.uuid4()}",
                pytest_results=pytest_results,
                artifacts=[]
            )
            
            return build_result
            
        except Exception as e:
            logger.error(f"Cloud Build execution failed: {e}")
            return CloudBuildResult(
                build_id=f"build-error-{uuid.uuid4()}",
                status="failure",
                duration=0.0,
                log_url="",
                pytest_results=None,
                artifacts=[]
            )
    
    def analyze_test_failures(self, pytest_results: PyTestResults) -> List[str]:
        """Analyze test failures and provide actionable feedback"""
        issues = []
        
        for test_result in pytest_results.test_results:
            if test_result.status in ['failed', 'error']:
                if test_result.error_message:
                    # Analyze common failure patterns
                    error_msg = test_result.error_message.lower()
                    
                    if 'assertionerror' in error_msg or 'assert' in error_msg:
                        issues.append(f"Assertion failure in {test_result.test_name}: {test_result.error_message}")
                    elif 'importerror' in error_msg or 'modulenotfounderror' in error_msg:
                        issues.append(f"Import error in {test_result.test_name}: Missing dependency or incorrect import")
                    elif 'attributeerror' in error_msg:
                        issues.append(f"Attribute error in {test_result.test_name}: Method or property not found")
                    elif 'typeerror' in error_msg:
                        issues.append(f"Type error in {test_result.test_name}: Incorrect argument types or signature")
                    elif 'syntaxerror' in error_msg:
                        issues.append(f"Syntax error in {test_result.test_name}: Code has syntax issues")
                    else:
                        issues.append(f"Test failure in {test_result.test_name}: {test_result.error_message}")
                else:
                    issues.append(f"Unknown failure in {test_result.test_name}")
        
        # Add coverage issues if coverage is too low
        if pytest_results.coverage_percentage and pytest_results.coverage_percentage < 70:
            issues.append(f"Low test coverage: {pytest_results.coverage_percentage}% (target: 70%+)")
        
        return issues
    
    def generate_improvement_suggestions(self, issues: List[str], pytest_results: PyTestResults) -> List[str]:
        """Generate suggestions for code improvement based on test results"""
        suggestions = []
        
        # Analyze failure patterns
        failure_types = {
            'assertion': 0,
            'import': 0,
            'type': 0,
            'syntax': 0,
            'attribute': 0
        }
        
        for issue in issues:
            issue_lower = issue.lower()
            if 'assertion' in issue_lower:
                failure_types['assertion'] += 1
            elif 'import' in issue_lower:
                failure_types['import'] += 1
            elif 'type' in issue_lower:
                failure_types['type'] += 1
            elif 'syntax' in issue_lower:
                failure_types['syntax'] += 1
            elif 'attribute' in issue_lower:
                failure_types['attribute'] += 1
        
        # Generate targeted suggestions
        if failure_types['assertion'] > 0:
            suggestions.append("Review assertion logic and expected vs actual values")
        
        if failure_types['import'] > 0:
            suggestions.append("Check import statements and ensure all dependencies are included")
        
        if failure_types['type'] > 0:
            suggestions.append("Add proper type hints and validate function signatures")
        
        if failure_types['syntax'] > 0:
            suggestions.append("Review code syntax, indentation, and Python grammar")
        
        if failure_types['attribute'] > 0:
            suggestions.append("Verify class methods and properties are correctly implemented")
        
        # Coverage suggestions
        if pytest_results.coverage_percentage and pytest_results.coverage_percentage < 70:
            suggestions.append("Add more comprehensive test cases to improve coverage")
        
        # Performance suggestions
        if pytest_results.duration > 30:
            suggestions.append("Consider optimizing slow tests or adding test parallelization")
        
        return suggestions
    
    async def send_feedback_to_dev_agent(self, feedback: ReviewFeedback, request_id: str) -> bool:
        """Send feedback to DevAgent for code improvement"""
        if not request_id:
            logger.warning("No DevAgent request ID provided, skipping feedback")
            return False
        
        try:
            feedback_payload = {
                "request_id": request_id,
                "review_passed": feedback.review_passed,
                "issues": feedback.issues_found,
                "suggestions": feedback.suggestions,
                "code_quality_score": feedback.code_quality_score,
                "retry_recommended": feedback.retry_recommended
            }
            
            async with httpx.AsyncClient() as client:
                response = await client.post(
                    f"{self.dev_agent_url}/feedback",
                    json=feedback_payload,
                    timeout=30.0
                )
                
                if response.status_code == 200:
                    logger.info(f"Successfully sent feedback to DevAgent for request {request_id}")
                    return True
                else:
                    logger.error(f"DevAgent feedback failed: {response.status_code}")
                    return False
                    
        except Exception as e:
            logger.error(f"Error sending feedback to DevAgent: {e}")
            return False
    
    async def add_github_pr_comment(
        self, 
        pr_number: int, 
        review_result: CodeReviewResult,
        tenant_context: TenantContext
    ) -> bool:
        """Add a comment to GitHub PR with review results"""
        if not self.github_integration:
            logger.warning("GitHub integration not available - skipping PR comment")
            return False
        
        if not self.auto_comment_enabled:
            logger.info("Auto PR commenting disabled")
            return False
        
        try:
            # Generate comprehensive review comment
            comment_parts = [
                f"## ðŸ¤– ReviewAgent Analysis - {review_result.module_name}",
                "",
                f"**Review Status:** {'âœ… PASSED' if review_result.review_status == 'passed' else 'âŒ FAILED'}",
                f"**Quality Score:** {review_result.feedback.code_quality_score:.1f}/100",
                ""
            ]
            
            # Add test results if available
            if review_result.cloud_build_result and review_result.cloud_build_result.pytest_results:
                pytest_results = review_result.cloud_build_result.pytest_results
                comment_parts.extend([
                    "### ðŸ§ª Test Results",
                    f"- **Total Tests:** {pytest_results.total_tests}",
                    f"- **Passed:** {pytest_results.passed} âœ…",
                    f"- **Failed:** {pytest_results.failed} âŒ",
                    f"- **Skipped:** {pytest_results.skipped} â­ï¸",
                    f"- **Duration:** {pytest_results.duration:.2f}s",
                    ""
                ])
                
                if pytest_results.coverage_percentage:
                    coverage_emoji = "âœ…" if pytest_results.coverage_percentage >= 80 else "âš ï¸" if pytest_results.coverage_percentage >= 70 else "âŒ"
                    comment_parts.append(f"- **Coverage:** {pytest_results.coverage_percentage:.1f}% {coverage_emoji}")
                    comment_parts.append("")
            
            # Add issues found
            if review_result.feedback.issues_found:
                comment_parts.extend([
                    "### ðŸš¨ Issues Found",
                    ""
                ])
                for i, issue in enumerate(review_result.feedback.issues_found[:10], 1):  # Limit to 10 issues
                    comment_parts.append(f"{i}. {issue}")
                
                if len(review_result.feedback.issues_found) > 10:
                    comment_parts.append(f"... and {len(review_result.feedback.issues_found) - 10} more issues")
                comment_parts.append("")
            
            # Add suggestions
            if review_result.feedback.suggestions:
                comment_parts.extend([
                    "### ðŸ’¡ Suggestions",
                    ""
                ])
                for i, suggestion in enumerate(review_result.feedback.suggestions[:5], 1):  # Limit to 5 suggestions
                    comment_parts.append(f"{i}. {suggestion}")
                comment_parts.append("")
            
            # Add Cloud Build info if available
            if review_result.cloud_build_result:
                comment_parts.extend([
                    "### ðŸ—ï¸ Cloud Build",
                    f"- **Build ID:** `{review_result.cloud_build_result.build_id}`",
                    f"- **Status:** {review_result.cloud_build_result.status}",
                    f"- **Duration:** {review_result.cloud_build_result.duration:.2f}s",
                    ""
                ])
                
                if review_result.cloud_build_result.log_url:
                    comment_parts.append(f"- **[View Build Logs]({review_result.cloud_build_result.log_url})**")
                    comment_parts.append("")
            
            # Add next steps
            if review_result.review_status == "failed":
                comment_parts.extend([
                    "### ðŸ”„ Next Steps",
                    "1. Review the issues and suggestions above",
                    "2. Fix the identified problems",
                    "3. Push changes to trigger a new review",
                    ""
                ])
                
                if review_result.feedback.retry_recommended:
                    comment_parts.append("*DevAgent will automatically regenerate improved code based on this feedback.*")
            else:
                comment_parts.extend([
                    "### âœ… Ready for Merge",
                    "All tests passed and code quality checks are satisfied. This PR is ready for merge.",
                    ""
                ])
            
            comment_parts.extend([
                "---",
                f"*ðŸ¤– This review was performed automatically by ReviewAgent at {review_result.created_at.strftime('%Y-%m-%d %H:%M:%S')} UTC*",
                f"*Review ID: `{review_result.review_id}`*"
            ])
            
            comment_text = "\n".join(comment_parts)
            
            # Add the comment to the PR
            await self.github_integration.add_pr_comment(pr_number, comment_text)
            
            # Log the comment
            await self.tenant_db.log_agent_event(
                tenant_context=tenant_context,
                event_type="pr_comment_added",
                agent_name="ReviewAgent",
                stage="pr_commenting",
                status="completed",
                project_id=review_result.project_id,
                output_data={
                    "pr_number": pr_number,
                    "review_id": review_result.review_id,
                    "comment_length": len(comment_text),
                    "review_status": review_result.review_status
                }
            )
            
            logger.info(f"Added review comment to PR #{pr_number}")
            return True
            
        except Exception as e:
            logger.error(f"Error adding PR comment: {e}")
            
            # Log failure
            await self.tenant_db.log_agent_event(
                tenant_context=tenant_context,
                event_type="pr_comment_failed",
                agent_name="ReviewAgent",
                stage="pr_commenting",
                status="failed",
                project_id=review_result.project_id,
                error_message=str(e)
            )
            
            return False
    
    async def add_inline_pr_comments(
        self, 
        pr_number: int, 
        review_result: CodeReviewResult,
        tenant_context: TenantContext
    ) -> bool:
        """Add inline comments to specific files/lines in the PR"""
        if not self.github_integration:
            logger.warning("GitHub integration not available - skipping inline comments")
            return False
        
        try:
            # Extract issues that can be mapped to specific lines
            inline_comments = []
            
            for test_result in review_result.feedback.test_failures:
                if test_result.error_message and test_result.test_name:
                    # Try to determine which file the error relates to
                    for file in review_result.cloud_build_result.pytest_results.test_results if review_result.cloud_build_result else []:
                        if test_result.test_name in file.test_name:
                            comment = ReviewComment(
                                body=f"**Test Failure:** {test_result.error_message}\n\n*From test: `{test_result.test_name}`*",
                                path=None,  # Would need to parse traceback to get exact file/line
                                line=None,
                                side="RIGHT"
                            )
                            inline_comments.append(comment)
                            break
            
            # Add inline comments
            for comment in inline_comments[:5]:  # Limit to 5 inline comments
                await self.github_integration.add_pr_review_comment(pr_number, comment)
            
            logger.info(f"Added {len(inline_comments)} inline comments to PR #{pr_number}")
            return True
            
        except Exception as e:
            logger.error(f"Error adding inline PR comments: {e}")
            return False
    
    async def review_generated_code(self, request: CodeReviewRequest, tenant_context: TenantContext) -> CodeReviewResult:
        """Main method to review generated code using pytest in Cloud Build"""
        review_id = str(uuid.uuid4())
        start_time = datetime.utcnow()
        
        logger.info(f"Starting code review {review_id} for project {request.project_id}")
        
        # Log review start
        await self.tenant_db.log_agent_event(
            tenant_context=tenant_context,
            event_type="code_review",
            agent_name="ReviewAgent",
            stage="review_start",
            status="started",
            project_id=request.project_id,
            input_data=request.model_dump(exclude={"generated_files"})
        )
        
        workspace_dir = None
        try:
            # Setup test workspace
            workspace_dir = await self.setup_test_workspace(request.generated_files)
            
            # Run pytest in Cloud Build (or locally as fallback)
            if request.cloud_build_config:
                cloud_build_result = await self.run_pytest_cloud_build(workspace_dir, request.cloud_build_config)
            else:
                pytest_results = await self.run_pytest_locally(workspace_dir)
                cloud_build_result = CloudBuildResult(
                    build_id=f"local-{uuid.uuid4()}",
                    status="success" if pytest_results.exit_code == 0 else "failure",
                    duration=pytest_results.duration,
                    log_url="",
                    pytest_results=pytest_results,
                    artifacts=[]
                )
            
            # Analyze results and generate feedback
            pytest_results = cloud_build_result.pytest_results or PyTestResults(
                total_tests=0, passed=0, failed=1, skipped=0, errors=1,
                duration=0.0, test_results=[], exit_code=1,
                output="", error_output="No pytest results available"
            )
            
            issues = self.analyze_test_failures(pytest_results)
            suggestions = self.generate_improvement_suggestions(issues, pytest_results)
            
            # Calculate quality score
            quality_score = 0.0
            if pytest_results.total_tests > 0:
                pass_rate = pytest_results.passed / pytest_results.total_tests
                quality_score = pass_rate * 100
                
                # Adjust for coverage
                if pytest_results.coverage_percentage:
                    quality_score = (quality_score * 0.7) + (pytest_results.coverage_percentage * 0.3)
            
            # Determine if review passed
            review_passed = (
                pytest_results.exit_code == 0 and
                pytest_results.failed == 0 and
                pytest_results.errors == 0 and
                quality_score >= 70
            )
            
            # Create feedback
            feedback = ReviewFeedback(
                review_passed=review_passed,
                issues_found=issues,
                test_failures=[r for r in pytest_results.test_results if r.status in ['failed', 'error']],
                suggestions=suggestions,
                code_quality_score=quality_score,
                retry_recommended=not review_passed and len(issues) > 0
            )
            
            # Send feedback to DevAgent if review failed
            if not review_passed and request.dev_agent_request_id:
                await self.send_feedback_to_dev_agent(feedback, request.dev_agent_request_id)
            
            # Create result
            review_duration = (datetime.utcnow() - start_time).total_seconds()
            result = CodeReviewResult(
                review_id=review_id,
                project_id=request.project_id,
                module_name=request.module_name,
                review_status="passed" if review_passed else "failed",
                cloud_build_result=cloud_build_result,
                feedback=feedback,
                review_duration=review_duration,
                created_at=start_time
            )
            
            # Log completion
            await self.tenant_db.log_agent_event(
                tenant_context=tenant_context,
                event_type="code_review",
                agent_name="ReviewAgent",
                stage="review_complete",
                status="completed" if review_passed else "failed",
                project_id=request.project_id,
                output_data=result.model_dump(exclude={"cloud_build_result", "feedback.test_failures"})
            )
            
            return result
            
        except Exception as e:
            logger.error(f"Code review failed: {e}")
            
            # Log failure
            await self.tenant_db.log_agent_event(
                tenant_context=tenant_context,
                event_type="code_review",
                agent_name="ReviewAgent",
                stage="review_error",
                status="failed",
                project_id=request.project_id,
                error_message=str(e)
            )
            
            # Return error result
            review_duration = (datetime.utcnow() - start_time).total_seconds()
            return CodeReviewResult(
                review_id=review_id,
                project_id=request.project_id,
                module_name=request.module_name,
                review_status="error",
                cloud_build_result=None,
                feedback=ReviewFeedback(
                    review_passed=False,
                    issues_found=[f"Review error: {str(e)}"],
                    test_failures=[],
                    suggestions=["Check ReviewAgent logs for detailed error information"],
                    code_quality_score=0.0,
                    retry_recommended=True
                ),
                review_duration=review_duration,
                created_at=start_time
            )
            
        finally:
            # Cleanup workspace
            if workspace_dir and os.path.exists(workspace_dir):
                shutil.rmtree(workspace_dir, ignore_errors=True)

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    logger.info("Starting Review Agent")
    yield
    # Shutdown
    logger.info("Shutting down Review Agent")

app = FastAPI(
    title="Review Agent",
    description="Code review agent with Cloud Build pytest integration",
    version="1.0.0",
    lifespan=lifespan,
)

# Add CORS middleware
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# Initialize agent
review_agent = ReviewAgent()

async def get_tenant_context(request) -> TenantContext:
    """Get tenant context from request headers"""
    headers = dict(request.headers)
    tenant_context = get_tenant_context_from_headers(headers)
    if not tenant_context:
        # For development, use default tenant
        tenant_context = TenantContext(tenant_id="default", user_id="review-agent")
    return tenant_context

@app.get("/")
async def root():
    """Root endpoint"""
    return {"message": "Review Agent v1.0", "status": "running"}

@app.get("/health")
async def health_check():
    """Health check endpoint"""
    return {"status": "OK", "agent": "review", "version": "1.0.0"}

@app.post("/review", response_model=CodeReviewResult)
async def review_code(
    request: CodeReviewRequest,
    tenant_context: TenantContext = Depends(get_tenant_context)
):
    """Review generated code using pytest in Cloud Build"""
    return await review_agent.review_generated_code(request, tenant_context)

@app.post("/review-feedback")
async def submit_review_feedback(
    feedback: ReviewFeedback,
    tenant_context: TenantContext = Depends(get_tenant_context)
):
    """Submit manual review feedback"""
    # Log manual feedback
    await review_agent.tenant_db.log_agent_event(
        tenant_context=tenant_context,
        event_type="manual_review",
        agent_name="ReviewAgent",
        stage="manual_feedback",
        status="completed",
        input_data=feedback.model_dump()
    )
    
    return {"message": "Feedback submitted successfully"}

@app.get("/review-history/{project_id}")
async def get_review_history(
    project_id: str,
    limit: int = 10,
    tenant_context: TenantContext = Depends(get_tenant_context)
):
    """Get review history for a project"""
    try:
        events = await review_agent.tenant_db.get_tenant_events(
            tenant_context=tenant_context,
            event_type="code_review"
        )
        
        # Filter by project and limit
        project_events = [e for e in events if e.get('project_id') == project_id][:limit]
        
        return {"project_id": project_id, "reviews": project_events}
        
    except Exception as e:
        logger.error(f"Error getting review history: {e}")
        raise HTTPException(status_code=500, detail=str(e))

@app.post("/pr-comment")
async def add_pr_comment(
    pr_comment_request: Dict[str, Any],
    tenant_context: TenantContext = Depends(get_tenant_context)
):
    """Add a comment to a GitHub PR with review results"""
    try:
        pr_number = pr_comment_request.get("pr_number")
        review_result_data = pr_comment_request.get("review_result")
        
        if not pr_number or not review_result_data:
            raise HTTPException(status_code=400, detail="pr_number and review_result are required")
        
        # Convert review result data to CodeReviewResult
        review_result = CodeReviewResult(**review_result_data)
        
        # Add the comment
        success = await review_agent.add_github_pr_comment(pr_number, review_result, tenant_context)
        
        return {
            "message": "PR comment added successfully" if success else "Failed to add PR comment",
            "success": success,
            "pr_number": pr_number
        }
        
    except Exception as e:
        logger.error(f"Error adding PR comment: {e}")
        raise HTTPException(status_code=500, detail=str(e))

if __name__ == "__main__":
    import uvicorn
    uvicorn.run(app, host="0.0.0.0", port=8084) 